{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# üéì Week 1 Mini-Project: Learning Assistant\n",
    "\n",
    "Welcome to your first personal research tool! This assistant will help you master any topic by analyzing multiple data sources and providing answers with clear citations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### 1. Setup & Imports\n",
    "We need to point Python to our `src` folder, which is now **two levels up**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM_PROVIDER: groq\n",
      "EMBEDDING_PROVIDER: local\n",
      "EMBEDDING_MODEL: multi-qa-distilbert-cos-v1\n",
      "VECTOR_SIZE: 768\n",
      "USE_INTELLIGENT_CHUNKING: False\n",
      "Initializing collection: research_documents\n",
      "‚úÖ Assistant initialized and ready!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path (two levels up from projects/week1_learning_assistant/)\n",
    "sys.path.append(os.path.abspath(os.path.join('..', '..')))\n",
    "\n",
    "from src.utils.config import Config\n",
    "from src.document_loader import DocumentLoader, Document\n",
    "from src.vector_store import VectorStore\n",
    "from groq import Groq\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Initialize components\n",
    "vector_store = VectorStore(in_memory=False)\n",
    "groq_client = Groq(api_key=Config.GROQ_API_KEY)\n",
    "loader = DocumentLoader()\n",
    "\n",
    "print(\"‚úÖ Assistant initialized and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 2. The Ingestion Engine\n",
    "This function handles loading and indexing all your research materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_learning_material(topic: str, sources: dict):\n",
    "    \"\"\"\n",
    "    Ingests research materials for a specific topic.\n",
    "    Sources dict: {\"pdfs\": [], \"webs\": [], \"youtubes\": []}\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting Ingestion for Topic: {topic.upper()}\")\n",
    "    \n",
    "    # Clear previous topic data\n",
    "    vector_store.clear()\n",
    "    \n",
    "    all_docs = []\n",
    "    project_root = Config.PROJECT_ROOT\n",
    "\n",
    "        # 1. Process GitHub Repos (Smart Parsing for URLs and Shorthand)\n",
    "    for repo_info in sources.get(\"githubs\", []):\n",
    "        # Clean the input: Remove 'https://github.com/' if it's there\n",
    "        clean_info = repo_info.replace(\"https://github.com/\", \"\").strip(\"/\")\n",
    "        print(\"clean_info\", clean_info)\n",
    "        parts = clean_info.split('/')\n",
    "        print(\"parts\", parts)\n",
    "        \n",
    "        if len(parts) < 2:\n",
    "            print(f\"   ‚ö†Ô∏è Skipping invalid GitHub info: {repo_info}\")\n",
    "            continue\n",
    "            \n",
    "        owner = parts[0]\n",
    "        name = parts[1]\n",
    "        # Use provided branch, or default to 'main'\n",
    "        branch = parts[2] if len(parts) > 2 else \"main\" \n",
    "        \n",
    "        print(f\"   üìÇ Downloading Github Repo: {owner}/{name} (Branch: {branch})\")\n",
    "        repo_docs = loader.load_github_repo(owner, name, branch)\n",
    "        for d in repo_docs:\n",
    "            d.metadata[\"topic\"] = topic\n",
    "        all_docs.extend(repo_docs)\n",
    "    \n",
    "    # Load PDFs\n",
    "    for path in sources.get(\"pdfs\", []):\n",
    "        # Handle relative paths from project root\n",
    "        full_path = os.path.join(project_root, path) if not os.path.isabs(path) else path\n",
    "        if os.path.exists(full_path):\n",
    "            print(f\"   üìÑ Loading PDF: {os.path.basename(full_path)}\")\n",
    "            pdf_pages = loader.load_pdf(full_path)\n",
    "            for p in pdf_pages:\n",
    "                p.metadata[\"topic\"] = topic\n",
    "            all_docs.extend(pdf_pages)\n",
    "            \n",
    "    # Load Web Pages\n",
    "    for url in sources.get(\"webs\", []):\n",
    "        print(f\"   üåê Loading Web: {url}\")\n",
    "        web_doc = loader.load_web_page(url)\n",
    "        web_doc.metadata[\"topic\"] = topic\n",
    "        all_docs.append(web_doc)\n",
    "        \n",
    "    # Load YouTube Transcripts\n",
    "    for url in sources.get(\"youtubes\", []):\n",
    "        print(f\"   üé• Loading YouTube: {url}\")\n",
    "        yt_doc = loader.load_youtube_transcript(url)\n",
    "        yt_doc.metadata[\"topic\"] = topic\n",
    "        all_docs.append(yt_doc)\n",
    "        \n",
    "    if all_docs:\n",
    "        count = vector_store.add_documents(all_docs)\n",
    "        print(f\"\\n‚úÖ Knowledge Base Updated! {count} research chunks ready for '{topic}'.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No materials found to index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 3. Topic Definition\n",
    "Choose a topic and provide your sources here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Ingestion for Topic: DOCKER FUNDAMENTALS\n",
      "Initializing collection: research_documents\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m MY_TOPIC = \u001b[33m\"\u001b[39m\u001b[33mDocker Fundamentals\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m SOURCES = {\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpdfs\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mdata/docker_cheatsheet.pdf\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mwebs\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mhttps://docs.docker.com/get-started/overview/\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33myoutubes\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mhttps://www.youtube.com/watch?v=fqMOX6JJhGo\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgithubs\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mdocker/cli/master\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;66;03m# Example: Indexing the official Docker CLI repo docs\u001b[39;00m\n\u001b[32m      8\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mingest_learning_material\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMY_TOPIC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOURCES\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mingest_learning_material\u001b[39m\u001b[34m(topic, sources)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müöÄ Starting Ingestion for Topic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Clear previous topic data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m all_docs = []\n\u001b[32m     12\u001b[39m project_root = Config.PROJECT_ROOT\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\personal-research-assistant-with-multi-source-rag\\ai-research-assistant\\src\\vector_store.py:240\u001b[39m, in \u001b[36mVectorStore.clear\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Clear all data from collection\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[38;5;28mself\u001b[39m.qdrant_client.delete_collection(\u001b[38;5;28mself\u001b[39m.collection_name)\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[OK] Cleared collection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.collection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\personal-research-assistant-with-multi-source-rag\\ai-research-assistant\\src\\vector_store.py:57\u001b[39m, in \u001b[36mVectorStore._initialize_collection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     54\u001b[39m collection_names = [c.name \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m collections]\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.collection_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m collection_names:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mqdrant_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvectors_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVectorParams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVECTOR_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDistance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCOSINE\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[OK] Created collection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.collection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\personal-research-assistant-with-multi-source-rag\\ai-research-assistant\\.venv\\Lib\\site-packages\\qdrant_client\\qdrant_client.py:1694\u001b[39m, in \u001b[36mQdrantClient.create_collection\u001b[39m\u001b[34m(self, collection_name, vectors_config, sparse_vectors_config, shard_number, sharding_method, replication_factor, write_consistency_factor, on_disk_payload, hnsw_config, optimizers_config, wal_config, quantization_config, timeout, strict_mode_config, metadata, **kwargs)\u001b[39m\n\u001b[32m   1644\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create empty collection with given parameters\u001b[39;00m\n\u001b[32m   1645\u001b[39m \n\u001b[32m   1646\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1690\u001b[39m \u001b[33;03m    Operation result\u001b[39;00m\n\u001b[32m   1691\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1692\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) == \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvectors_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvectors_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshard_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m    \u001b[49m\u001b[43msharding_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharding_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreplication_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreplication_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_consistency_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrite_consistency_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_disk_payload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_disk_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhnsw_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhnsw_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwal_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43msparse_vectors_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43msparse_vectors_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstrict_mode_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict_mode_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\personal-research-assistant-with-multi-source-rag\\ai-research-assistant\\.venv\\Lib\\site-packages\\qdrant_client\\local\\qdrant_local.py:796\u001b[39m, in \u001b[36mQdrantLocal.create_collection\u001b[39m\u001b[34m(self, collection_name, vectors_config, sparse_vectors_config, metadata, **kwargs)\u001b[39m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collection_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    794\u001b[39m     os.makedirs(collection_path, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m collection = \u001b[43mLocalCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrest_models\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCreateCollection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvectors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvectors_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m        \u001b[49m\u001b[43msparse_vectors\u001b[49m\u001b[43m=\u001b[49m\u001b[43msparse_vectors_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_disable_check_same_thread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforce_disable_check_same_thread\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[38;5;28mself\u001b[39m.collections[collection_name] = collection\n\u001b[32m    807\u001b[39m \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\personal-research-assistant-with-multi-source-rag\\ai-research-assistant\\.venv\\Lib\\site-packages\\qdrant_client\\local\\local_collection.py:148\u001b[39m, in \u001b[36mLocalCollection.__init__\u001b[39m\u001b[34m(self, config, location, force_disable_check_same_thread)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mself\u001b[39m.storage = CollectionPersistence(location, force_disable_check_same_thread)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\personal-research-assistant-with-multi-source-rag\\ai-research-assistant\\.venv\\Lib\\site-packages\\qdrant_client\\local\\local_collection.py:211\u001b[39m, in \u001b[36mLocalCollection.load_vectors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m multivectors = defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[32m    209\u001b[39m deleted_ids = []\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# id tracker\u001b[39;49;00m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# no gaps in idx\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learning\\personal-research-assistant-with-multi-source-rag\\ai-research-assistant\\.venv\\Lib\\site-packages\\qdrant_client\\local\\persistence.py:152\u001b[39m, in \u001b[36mCollectionPersistence.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    150\u001b[39m cursor = \u001b[38;5;28mself\u001b[39m.storage.cursor()\n\u001b[32m    151\u001b[39m cursor.execute(\u001b[33m\"\u001b[39m\u001b[33mSELECT point FROM points\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m pickle.loads(row[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "MY_TOPIC = \"Docker Fundamentals\"\n",
    "\n",
    "SOURCES = {\n",
    "    \"pdfs\": [\"data/docker_cheatsheet.pdf\"],\n",
    "    \"webs\": [\"https://docs.docker.com/get-started/overview/\"],\n",
    "    \"youtubes\": [\"https://www.youtube.com/watch?v=fqMOX6JJhGo\"],\n",
    "    \"githubs\": [\"docker/cli/master\"], # Example: Indexing the official Docker CLI repo docs\n",
    "}\n",
    "\n",
    "ingest_learning_material(MY_TOPIC, SOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10a3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_assistant(question: str):\n",
    "    \"\"\"\n",
    "    Searches knowledge base and generates answer with clear citations.\n",
    "    \"\"\"\n",
    "    # 1. Search for relevant information\n",
    "    results = vector_store.search(question, top_k=3)\n",
    "    \n",
    "    if not results:\n",
    "        display(Markdown(\"‚ö†Ô∏è *I couldn't find any information about that in your materials.*\"))\n",
    "        return\n",
    "\n",
    "    # 2. Process Context and unique Sources\n",
    "    context_parts = []\n",
    "    sources = []\n",
    "    \n",
    "    for res in results:\n",
    "        context_parts.append(res['text'])\n",
    "        \n",
    "        # Build a citation string\n",
    "        meta = res['metadata']\n",
    "        source_type = meta.get('source_type', 'unknown').upper()\n",
    "        # Find the name from path or url\n",
    "        name = os.path.basename(meta.get('source_path', '')) or meta.get('source_url', 'Web/YouTube')\n",
    "        page = f\" (Page {meta['page_number']})\" if 'page_number' in meta else \"\"\n",
    "        citation = f\"{source_type}: {name}{page}\"\n",
    "        \n",
    "        if citation not in sources:\n",
    "            sources.append(citation)\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # 3. Ask the AI (Groq)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a professional Learning Assistant. Answer questions accurately using ONLY the provided context. Use bullet points for readability if appropriate.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context_text}\\n\\nQuestion: {question}\"}\n",
    "    ]\n",
    "    \n",
    "    # Standard non-streaming call for simplicity and reliability\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=Config.GROQ_MODEL,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    # 4. Display the beautiful output\n",
    "    answer = response.choices[0].message.content\n",
    "    display(Markdown(f\"### ü§ñ Assistant Answer\\n{answer}\"))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"üìö SOURCES USED FOR THIS ANSWER:\")\n",
    "    for i, source in enumerate(sources, 1):\n",
    "        print(f\"{i}. {source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "057323bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Welcome to the Docker Fundamentals Classroom ---\n",
      "(Type 'exit' or 'quit' to finish your session)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ü§ñ Assistant Answer\n",
       "Here are the benefits of Docker:\n",
       "* Fast and consistent delivery of applications\n",
       "* Streamlines the development lifecycle\n",
       "* Allows developers to work in standardized environments\n",
       "* Enables continuous integration and continuous delivery (CI/CD) workflows\n",
       "* Provides a loosely isolated environment for applications, ensuring security and stability\n",
       "* Enables running many containers simultaneously on a given host\n",
       "* Containers are lightweight and contain everything needed to run the application\n",
       "* No need to rely on what's installed on the host\n",
       "* Enables sharing of containers while working, ensuring everyone gets the same container that works in the same way\n",
       "* Reduces the delay between writing code and running it in production\n",
       "* Allows managing infrastructure in the same ways as managing applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üìö SOURCES USED FOR THIS ANSWER:\n",
      "1. WEB: https://docs.docker.com/get-started/overview/\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### ü§ñ Assistant Answer\n",
       "To spin up a Docker container, you can use the following command:\n",
       "\n",
       "```console\n",
       "$ docker run -it <image_name>\n",
       "```\n",
       "\n",
       " Replace `<image_name>` with the name of the image you want to use. For example:\n",
       "\n",
       "```console\n",
       "$ docker run -it alpine\n",
       "```\n",
       "\n",
       "or \n",
       "\n",
       "```console\n",
       "$ docker run -it debian\n",
       "```\n",
       "\n",
       "This will create and start a new container from the specified image, with an interactive shell and a pseudo-TTY attached. \n",
       "\n",
       "Alternatively, you can also specify a name for the container and create it before starting it:\n",
       "\n",
       "```console\n",
       "$ docker container create -i -t --name mycontainer <image_name>\n",
       "$ docker container start --attach -i mycontainer\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üìö SOURCES USED FOR THIS ANSWER:\n",
      "1. GITHUB: https://github.com/docker/cli/blob/master/docs/reference/commandline/container_run.md\n",
      "2. GITHUB: https://github.com/docker/cli/blob/master/docs/reference/commandline/container_create.md\n",
      "3. GITHUB: https://github.com/docker/cli/blob/master/docs/reference/commandline/container_attach.md\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "üëã Happy studying! Closing the session.\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- Welcome to the {MY_TOPIC} Classroom ---\")\n",
    "print(\"(Type 'exit' or 'quit' to finish your session)\\n\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"What would you like to learn about? \")\n",
    "    \n",
    "    if user_query.lower() in ['exit', 'quit']:\n",
    "        print(\"\\nüëã Happy studying! Closing the session.\")\n",
    "        break\n",
    "    \n",
    "    if not user_query.strip():\n",
    "        continue\n",
    "        \n",
    "    query_assistant(user_query)\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c0682",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI Research Assistant)",
   "language": "python",
   "name": "ai-assistant-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
